{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'normalize'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mImportError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-c2161486fab4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtrain_vgg19\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mvgg19\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mload_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnormalize\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mNormalize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'normalize'"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import multiprocessing as mp\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.autograd import Variable\n",
    "import torchvision\n",
    "from torchvision.transforms import transforms\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "import time\n",
    "from models import modelA\n",
    "from train_vgg19 import vgg19\n",
    "from utils import get_data, load_list, save_list\n",
    "from utils import normalize as Normalize\n",
    "import warnings\n",
    "\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.neighbors import KernelDensity\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import scipy.io as sio\n",
    "from sklearn.preprocessing import scale\n",
    "from advertorch.attacks import PGDAttack, GradientSignAttack, LinfBasicIterativeAttack, \\\n",
    "                                    CarliniWagnerL2Attack, DDNL2Attack, SinglePixelAttack, JacobianSaliencyMapAttack\n",
    "from utils import CW\n",
    "\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "\n",
    "device = 'cuda:3' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flip(x, nb_diff):\n",
    "    original_shape = x.shape\n",
    "    x = np.copy(np.reshape(x, (-1,)))\n",
    "    candidate_inds = np.where(x < 0.99)[0]\n",
    "    assert candidate_inds.shape[0] >= nb_diff\n",
    "    inds = np.random.choice(candidate_inds, nb_diff)\n",
    "    x[inds] = 1.\n",
    "\n",
    "    return np.reshape(x, original_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifier_adv(data):\n",
    "        return F.log_softmax(model(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_noisy_samples(X_test, X_test_adv, dataset, attack):\n",
    "    if attack in ['jsma', 'cw']:\n",
    "        X_test_noisy = np.zeros_like(X_test)\n",
    "        for i in range(len(X_test)):\n",
    "            # Count the number of pixels that are different\n",
    "            nb_diff = len(np.where(X_test[i] != X_test_adv[i])[0])\n",
    "            # Randomly flip an equal number of pixels (flip means move to max\n",
    "            # value of 1)\n",
    "            X_test_noisy[i] = flip(X_test[i], nb_diff)\n",
    "    else:\n",
    "        warnings.warn(\"Using pre-set Gaussian scale sizes to craft noisy \"\n",
    "                      \"samples. If you've altered the eps/eps-iter parameters \"\n",
    "                      \"of the attacks used, you'll need to update these. In \"\n",
    "                      \"the future, scale sizes will be inferred automatically \"\n",
    "                      \"from the adversarial samples.\")\n",
    "        # Add Gaussian noise to the samples\n",
    "        X_test_noisy = np.minimum(\n",
    "            np.maximum(\n",
    "                X_test + np.random.normal(loc=0, scale=STDEVS[dataset][attack],\n",
    "                                          size=X_test.shape),\n",
    "                0\n",
    "            ),\n",
    "            1\n",
    "        )\n",
    "\n",
    "    return X_test_noisy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mc_predictions(model, X, nb_iter=50, batch_size=256):\n",
    "    model.train()\n",
    "    if dataset == 'mnist':\n",
    "        output_dim = list(model.children())[-1].out_features\n",
    "    else:\n",
    "        output_dim = list(model.children())[-1][-1].out_features\n",
    "    get_output = lambda data : F.softmax(model(data))\n",
    "#     get_output = lambda data : model(data)\n",
    "    X = torch.Tensor(X).to(device)\n",
    "    \n",
    "    def predict():\n",
    "        n_batches = int(np.ceil(X.shape[0] / float(batch_size)))\n",
    "        output = np.zeros(shape=(len(X), output_dim))\n",
    "        for i in range(n_batches):\n",
    "            output[i * batch_size:(i + 1) * batch_size] = get_output(X[i * batch_size:(i + 1) * batch_size]).detach().cpu().numpy()\n",
    "        return output\n",
    "\n",
    "    preds_mc = []\n",
    "    for i in tqdm(range(nb_iter)):\n",
    "        preds_mc.append(predict())\n",
    "\n",
    "    return np.asarray(preds_mc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_deep_representations(model, X, batch_size=256):\n",
    "    model.eval()\n",
    "    X = torch.Tensor(X).to(device)\n",
    "    # mnist modelA last hidden layer \n",
    "#     output_dim = model.layers[-4].output.shape[-1].value\n",
    "    last_hidden_idx = -3\n",
    "    if dataset == 'mnist':\n",
    "        output_dim = list(model.children())[last_hidden_idx].out_features\n",
    "    else:\n",
    "        output_dim = list(model.children())[-1][last_hidden_idx].out_features\n",
    "\n",
    "    last_hidden_output = None\n",
    "\n",
    "    def last_hidden_hook(module, input_, output):\n",
    "        nonlocal last_hidden_output\n",
    "        last_hidden_output = output\n",
    "    \n",
    "    if dataset == 'mnist':\n",
    "        list(model.children())[last_hidden_idx].register_forward_hook(last_hidden_hook)\n",
    "    else:\n",
    "        list(model.children())[-1][last_hidden_idx+1].register_forward_hook(last_hidden_hook)\n",
    "\n",
    "    n_batches = int(np.ceil(X.shape[0] / float(batch_size)))\n",
    "    output = np.zeros(shape=(len(X), output_dim))\n",
    "    for i in tqdm(range(n_batches)):\n",
    "#         output[i * batch_size:(i + 1) * batch_size] = get_encoding([X[i * batch_size:(i + 1) * batch_size], 0])[0]\n",
    "        model(X[i * batch_size:(i + 1) * batch_size])\n",
    "        output[i * batch_size:(i + 1) * batch_size] = last_hidden_output.detach().cpu().numpy()\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_point(tup):\n",
    "    x, kde = tup\n",
    "    return kde.score_samples(np.reshape(x, (1, -1)))[0]\n",
    "\n",
    "def score_samples(kdes, samples, preds, n_jobs=None):\n",
    "    if n_jobs is not None:\n",
    "        p = mp.Pool(n_jobs)\n",
    "    else:\n",
    "        p = mp.Pool()\n",
    "    \n",
    "    results = p.map(score_point, [(x, kdes[i]) for x, i in zip(samples, preds)])\n",
    "    p.close()\n",
    "    p.join()\n",
    "\n",
    "    return results\n",
    "\n",
    "# def normalize(normal, adv, noisy):\n",
    "#     n_samples = len(normal)\n",
    "#     total = scale(np.concatenate((normal, adv, noisy)))\n",
    "\n",
    "#     return total[:n_samples], total[n_samples:2*n_samples], total[2*n_samples:]\n",
    "\n",
    "def normalize(normal, adv):\n",
    "    n_samples = len(normal)\n",
    "    total = scale(np.concatenate((normal, adv)))\n",
    "\n",
    "    return total[:n_samples], total[n_samples:2*n_samples]\n",
    "\n",
    "def train_lr(densities_pos, densities_neg, uncerts_pos, uncerts_neg):\n",
    "    \"\"\"\n",
    "    TODO\n",
    "    :param densities_pos:\n",
    "    :param densities_neg:\n",
    "    :param uncerts_pos:\n",
    "    :param uncerts_neg:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    values_neg = np.concatenate(\n",
    "        (densities_neg.reshape((1, -1)),\n",
    "         uncerts_neg.reshape((1, -1))),\n",
    "        axis=0).transpose([1, 0])\n",
    "    values_pos = np.concatenate(\n",
    "        (densities_pos.reshape((1, -1)),\n",
    "         uncerts_pos.reshape((1, -1))),\n",
    "        axis=0).transpose([1, 0])\n",
    "\n",
    "    values = np.concatenate((values_neg, values_pos))\n",
    "    labels = np.concatenate(\n",
    "        (np.zeros_like(densities_neg), np.ones_like(densities_pos)))\n",
    "\n",
    "    lr = LogisticRegressionCV(n_jobs=-1).fit(values, labels)\n",
    "\n",
    "    return values, labels, lr\n",
    "\n",
    "\n",
    "\n",
    "def compute_roc(probs_neg, probs_pos, plot=False):\n",
    "    \"\"\"\n",
    "    TODO\n",
    "    :param probs_neg:\n",
    "    :param probs_pos:\n",
    "    :param plot:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    probs = np.concatenate((probs_neg, probs_pos))\n",
    "    labels = np.concatenate((np.zeros_like(probs_neg), np.ones_like(probs_pos)))\n",
    "    fpr, tpr, _ = roc_curve(labels, probs)\n",
    "    auc_score = auc(fpr, tpr)\n",
    "    if plot:\n",
    "        plt.figure(figsize=(7, 6))\n",
    "        plt.plot(fpr, tpr, color='blue',\n",
    "                 label='ROC (AUC = %0.4f)' % auc_score)\n",
    "        plt.legend(loc='lower right')\n",
    "        plt.title(\"ROC Curve\")\n",
    "        plt.xlabel(\"FPR\")\n",
    "        plt.ylabel(\"TPR\")\n",
    "        plt.show()\n",
    "\n",
    "    return fpr, tpr, auc_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameter setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading mnist data set\n"
     ]
    }
   ],
   "source": [
    "attack = 'fgsm'\n",
    "dataset = 'mnist'\n",
    "\n",
    "print('Loading %s data set' % dataset)\n",
    "if dataset == 'mnist':\n",
    "#     org_train_path = '../gen/adv_data/v1/mnist/org/mnist_org_data/mnist_org_train_7944.pkl'\n",
    "#     org_test_path = '../gen/adv_data/v1/mnist/org/mnist_org_data/mnist_org_test_1987.pkl'\n",
    "    org_train_path = '../gen/adv_data/v2/mnist/org/mnist_org_data/mnist_org_train_24000.pkl'\n",
    "    org_test_path = '../gen/adv_data/v2/mnist/org/mnist_org_data/mnist_org_test_6000.pkl'\n",
    "elif dataset == 'cifar':\n",
    "    org_train_path = '../gen/adv_data/v1/cifar10/org/cifar10_org_data/cifar10_org_train_7349.pkl'\n",
    "    org_test_path = '../gen/adv_data/v1/cifar10/org/cifar10_org_data/cifar10_org_test_1838.pkl'\n",
    "else:\n",
    "    # Imagenet\n",
    "    org_train_path = '' \n",
    "    org_test_path =  ''\n",
    "    \n",
    "batch_size = 100\n",
    "class_num = 10\n",
    "eps = 0.3\n",
    "\n",
    "# Determines perturbation of noisy samples\n",
    "STDEVS = {\n",
    "    'mnist': {'fgsm': 0.310, 'pgd':0.234, 'cw':0.234},\n",
    "    'cifar': {'fgsm': 0.050, 'pgd':0.033, 'cw':0.033},\n",
    "}\n",
    "\n",
    "# Gaussian kernel bandwidth for kernel density estimation\n",
    "BANDWIDTHS = {'mnist': 1.20, 'cifar': 0.26}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attack type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attack type : fgsm\n"
     ]
    }
   ],
   "source": [
    "if attack == 'fgsm':\n",
    "    print('Attack type : fgsm')\n",
    "    adversary = GradientSignAttack(classifier_adv, loss_fn=nn.CrossEntropyLoss(), eps=eps,clip_min=0.0, clip_max=1.0)\n",
    "elif attack == 'pgd':\n",
    "    print('Attack type : pgd')\n",
    "    adversary = PGDAttack(classifier_adv, loss_fn=nn.CrossEntropyLoss(), eps=eps)\n",
    "elif attack == 'cw':\n",
    "    print('Attack type : cw')\n",
    "    adversary = CW(classifier_adv, radius=eps, class_num=class_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mnist Model loading complete!\n"
     ]
    }
   ],
   "source": [
    "# mnist\n",
    "if dataset == 'mnist':\n",
    "    model = modelA()\n",
    "    checkpoint = torch.load('../gen/models/mnist/modelA/modelA.pkl')\n",
    "    model.load_state_dict(checkpoint)\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "elif dataset == 'cifar':\n",
    "    model = vgg19()\n",
    "    model.features = torch.nn.DataParallel(model.features)\n",
    "    model.to(device)\n",
    "    checkpoint = torch.load('../gen/models/cifar10/vgg19/checkpoint_299.tar')\n",
    "    model.load_state_dict(checkpoint)\n",
    "    model.eval()\n",
    "else:\n",
    "    #Imagenet model loading\n",
    "    pass\n",
    "    \n",
    "print('%s Model loading complete!'%dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dataset == 'mnist':\n",
    "    X_train, Y_train = load_list(org_train_path)[0].reshape(-1, 1, 28, 28), load_list(org_train_path)[1]\n",
    "    X_test, Y_test = load_list(org_test_path)[0].reshape(-1, 1, 28, 28), load_list(org_test_path)[1]\n",
    "elif dataset == 'cifar':\n",
    "    X_train, Y_train = load_list(org_train_path)[0].reshape(-1, 3, 32, 32), load_list(org_train_path)[1]\n",
    "    X_test, Y_test = load_list(org_test_path)[0].reshape(-1, 3, 32, 32), load_list(org_test_path)[1]\n",
    "    X_train, X_test = Normalize(X_train), Normalize(X_test)\n",
    "else:\n",
    "    X_train, Y_train = load_list(org_train_path)[0].reshape(-1, 3, 32, 32), load_list(org_train_path)[1]\n",
    "    X_test, Y_test = load_list(org_test_path)[0].reshape(-1, 3, 32, 32), load_list(org_test_path)[1]\n",
    "    X_train, X_test = Normalize(X_train), Normalize(X_test)\n",
    "    \n",
    "X_test_adv = adversary.perturb(torch.Tensor(X_test).to(device), torch.Tensor(Y_test).to(device).long())\n",
    "X_test_adv = X_test_adv.detach().cpu().numpy()\n",
    "\n",
    "# Craft an equal number of noisy samples\n",
    "X_test_noisy = get_noisy_samples(X_test, X_test_adv, dataset, attack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((6000, 1, 28, 28), (8000, 1, 28, 28), (2000, 1, 28, 28), (8000,), (2000,))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_adv.shape, X_train.shape, X_test.shape,Y_train.shape, Y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model accuracy on the normal test set: 100.00%\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'bool' object has no attribute 'sum'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-985ea9242214>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mcorrect\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mY_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcorrect\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mdset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'bool' object has no attribute 'sum'"
     ]
    }
   ],
   "source": [
    "for s_type, dset in zip(['normal', 'noisy', 'adversarial'], [X_test, X_test_noisy, X_test_adv]):\n",
    "    model.eval()\n",
    "    if dataset == 'mnist':\n",
    "        dset = torch.Tensor(dset).to(device).reshape(-1, 1, 28, 28)\n",
    "    else:\n",
    "        dset = torch.Tensor(dset).to(device).reshape(-1, 3, 32, 32)\n",
    "        \n",
    "        \n",
    "    output = model(dset)\n",
    "    \n",
    "    pred = torch.argmax(output, 1)\n",
    "    \n",
    "    correct = (pred.detach().cpu().numpy() == Y_test).sum()\n",
    "    acc = correct / dset.shape[0]\n",
    "    \n",
    "    print(\"Model accuracy on the %s test set: %0.2f%%\" %\n",
    "              (s_type, 100 * acc))\n",
    "    #Compute and display average perturbation sizes which define noisy perturbation size\n",
    "    if not s_type == 'normal':\n",
    "        l2_diff = np.linalg.norm(dset.detach().cpu().numpy().reshape((len(X_test), -1)) - X_test.reshape((len(X_test), -1)), axis=1).mean()\n",
    "        print(\"Average L-2 perturbation size of the %s test set: %0.2f\" % (s_type, l2_diff))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torch.Tensor(X_test).to(device)\n",
    "preds_test = torch.argmax(model(data), 1).detach().cpu().numpy()\n",
    "inds_correct = np.where(preds_test == Y_test)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = X_test[inds_correct]\n",
    "# X_test_noisy = X_test_noisy[inds_correct]\n",
    "X_test_adv = X_test_adv[inds_correct]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian uncertainty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting Monte Carlo dropout variance predictions...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b7a93e2956c4babb02b9f2e6ea4a14b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=50), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40a8cc5573ec47c7b03525ca74f9df98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=50), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    " print('Getting Monte Carlo dropout variance predictions...')\n",
    "uncerts_normal = get_mc_predictions(model, X_test, batch_size=batch_size).var(axis=0).mean(axis=1)\n",
    "# uncerts_noisy = get_mc_predictions(model, X_test_noisy, batch_size=batch_size).var(axis=0).mean(axis=1)\n",
    "uncerts_adv = get_mc_predictions(model, X_test_adv, batch_size=batch_size).var(axis=0).mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0005858372816411122, 0.0075784068584915576)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uncerts_normal.mean(), uncerts_adv.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kernel Density estimation score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting deep feature representations...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b77cc7a995764d12ac90a9ee15b82b6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=80), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f65dc789ad94f00b3570a8423216f30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=20), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3023b7a0530a4d3aa9fa35e1df6db0bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=20), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print('Getting deep feature representations...')\n",
    "X_train_features = get_deep_representations(model, X_train, batch_size=batch_size)\n",
    "X_test_normal_features = get_deep_representations(model, X_test,batch_size=batch_size)\n",
    "# X_test_noisy_features = get_deep_representations(model, X_test_noisy, batch_size=batch_size)\n",
    "X_test_adv_features = get_deep_representations(model, X_test_adv, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-2.6006504176887977, -1.5195453637127647)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_normal_features.mean(), X_test_adv_features.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training KDEs...\n",
      "Training finished!\n"
     ]
    }
   ],
   "source": [
    "print('Training KDEs...')\n",
    "class_inds = {}\n",
    "Y_train = np.eye(10)[Y_train]\n",
    "for i in range(Y_train.shape[1]):\n",
    "#     class_inds[i] = Y_train[i]\n",
    "    class_inds[i] = np.where(Y_train.argmax(axis=1) == i)[0]\n",
    "kdes = {}\n",
    "warnings.warn(\"Using pre-set kernel bandwidths that were determined \"\n",
    "              \"optimal for the specific CNN models of the paper. If you've \"\n",
    "              \"changed your model, you'll need to re-optimize the \"\n",
    "              \"bandwidth.\")\n",
    "for i in range(Y_train.shape[1]):\n",
    "    kdes[i] = KernelDensity(kernel='gaussian', bandwidth=BANDWIDTHS[dataset]).fit(X_train_features[class_inds[i]])\n",
    "print('Training finished!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing model predictions...\n",
      "Computing prediction finished!\n"
     ]
    }
   ],
   "source": [
    "# Get model predictions\n",
    "print('Computing model predictions...')\n",
    "model.eval()\n",
    "preds_test_normal = torch.argmax(model(torch.Tensor(X_test).to(device)), 1).detach().cpu().numpy()\n",
    "# preds_test_noisy = torch.argmax(model(torch.Tensor(X_test_noisy).to(device)), 1).detach().cpu().numpy()\n",
    "preds_test_adv = torch.argmax(model(torch.Tensor(X_test_adv).to(device)), 1).detach().cpu().numpy()\n",
    "print('Computing prediction finished!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing densities...\n",
      "Computing densities finished!\n"
     ]
    }
   ],
   "source": [
    "# Get density estimates\n",
    "print('computing densities...')\n",
    "densities_normal = score_samples(\n",
    "    kdes,\n",
    "    X_test_normal_features,\n",
    "    preds_test_normal\n",
    ")\n",
    "# densities_noisy = score_samples(\n",
    "#     kdes,\n",
    "#     X_test_noisy_features,\n",
    "#     preds_test_noisy\n",
    "# )\n",
    "densities_adv = score_samples(\n",
    "    kdes,\n",
    "    X_test_adv_features,\n",
    "    preds_test_adv\n",
    ")\n",
    "\n",
    "print('Computing densities finished!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training start...\n",
      "Training end!\n"
     ]
    }
   ],
   "source": [
    "## Z-score the uncertainty and density values\n",
    "# uncerts_normal_z, uncerts_adv_z, uncerts_noisy_z = normalize(\n",
    "#     uncerts_normal,\n",
    "#     uncerts_adv,\n",
    "#     uncerts_noisy\n",
    "# )\n",
    "# densities_normal_z, densities_adv_z, densities_noisy_z = normalize(\n",
    "#     densities_normal,\n",
    "#     densities_adv,\n",
    "#     densities_noisy\n",
    "# )\n",
    "\n",
    "uncerts_normal_z, uncerts_adv_z = normalize(\n",
    "    uncerts_normal,\n",
    "    uncerts_adv,\n",
    ")\n",
    "densities_normal_z, densities_adv_z = normalize(\n",
    "    densities_normal,\n",
    "    densities_adv,\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "## Build detector\n",
    "# print('Training start...')\n",
    "\n",
    "# values, labels, lr = train_lr(\n",
    "#     densities_pos=densities_adv_z,\n",
    "#     densities_neg=np.concatenate((densities_normal_z, densities_noisy_z)),\n",
    "#     uncerts_pos=uncerts_adv_z,\n",
    "#     uncerts_neg=np.concatenate((uncerts_normal_z, uncerts_noisy_z))\n",
    "# )\n",
    "\n",
    "# print('Training end!')\n",
    "\n",
    "print('Training start...')\n",
    "\n",
    "values, labels, lr = train_lr(\n",
    "    densities_pos=densities_adv_z,\n",
    "    densities_neg=densities_normal_z,\n",
    "    uncerts_pos=uncerts_adv_z,\n",
    "    uncerts_neg=uncerts_normal_z\n",
    ")\n",
    "\n",
    "print('Training end!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detector ROC-AUC score: 0.9586\n"
     ]
    }
   ],
   "source": [
    "## Evaluate detector\n",
    "# Compute logistic regression model predictions\n",
    "probs = lr.predict_proba(values)[:, 1]\n",
    "# Compute AUC\n",
    "n_samples = len(X_test)\n",
    "# The first 2/3 of 'probs' is the negative class (normal and noisy samples),\n",
    "# and the last 1/3 is the positive class (adversarial samples).\n",
    "# fpr, tpr, auc_score = compute_roc(\n",
    "#     probs_neg=probs[:2 * n_samples],\n",
    "#     probs_pos=probs[2 * n_samples:]\n",
    "# )\n",
    "\n",
    "fpr, tpr, auc_score = compute_roc(\n",
    "    probs_neg=probs[:n_samples],\n",
    "    probs_pos=probs[n_samples:]\n",
    ")\n",
    "print('Detector ROC-AUC score: %0.4f' % auc_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = (probs>=0.5).astype('int')\n",
    "idx = (uncerts_adv_z / uncerts_noisy_z) > 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6965904969169388"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(pred == labels).sum() / pred.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False,  True,  True, ...,  True,  True, False])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "((uncerts_adv / uncerts_normal) > 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "cut = int(pred.shape[0] / 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_pred = pred[:cut]\n",
    "noisy_pred = pred[cut:cut*2]\n",
    "adv_pred = pred[cut*2:]\n",
    "normal_labels = labels[:cut]\n",
    "noisy_labels = labels[cut:cut*2]\n",
    "adv_labels = labels[cut*2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uncertainty : 0.9167573449401524\n",
      "Kernel densities : 0.9042437431991295\n"
     ]
    }
   ],
   "source": [
    "idx = (uncerts_adv / uncerts_normal) > 1\n",
    "print('Uncertainty :',idx.sum()/idx.shape[0])\n",
    "idx = (np.array(densities_adv) / np.array(densities_normal)) < 1\n",
    "print('Kernel densities :', idx.sum()/idx.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0006046953096136334, 0.0029852911732738186, 0.007750921423681355)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uncerts_normal.mean(), uncerts_noisy.mean(), uncerts_adv.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-400.5944232222921, -260.94075303422323)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(densities_adv).mean(), np.array(densities_normal).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-400.5944232222921, -260.94075303422323)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(densities_adv).mean(), np.array(densities_normal).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
